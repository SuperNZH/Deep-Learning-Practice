{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7.0_byModule.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNSFUcDRxX56uDAedWDd1XP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuperNZH/Deep-Learning-Practice/blob/main/Dive%20in%20DL/7.0_byModule.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wG2At_uQYud7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 继承Module"
      ],
      "metadata": {
        "id": "Pj8qvbJjRkh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(MLP, self).__init__(**kwargs)\n",
        "    self.hidden = nn.Linear(784, 256)\n",
        "    self.act = nn.ReLU() # act 是激活函数的意思\n",
        "    self.output = nn.Linear(256, 10)\n",
        "  # 定义前向运算，根据x计算返回所需要的模型输出\n",
        "  def forward(self, x):\n",
        "    a = self.act(self.hidden(x))\n",
        "    return self.output(a)\n",
        "\n",
        "# 不需要定义反向函数，是因为这个class里面系统会自动求梯度，然后自动生成反向传播的backward函数"
      ],
      "metadata": {
        "id": "LzmVpqPjfG_N"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(2,784)\n",
        "print(X)\n",
        "net = MLP()\n",
        "print(net)\n",
        "net(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygqnuiv1mIpn",
        "outputId": "cd402cb7-4a60-407f-ff35-44be30607a95"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4609, 0.1118, 0.2783,  ..., 0.7517, 0.3530, 0.0044],\n",
            "        [0.0986, 0.6646, 0.2299,  ..., 0.1502, 0.5865, 0.7586]])\n",
            "MLP(\n",
            "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (act): ReLU()\n",
            "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0737, -0.0027, -0.0751, -0.0714,  0.1377,  0.1214, -0.2058, -0.0618,\n",
              "          0.1190, -0.1268],\n",
              "        [-0.0682, -0.1824, -0.0720, -0.1660,  0.0308,  0.0348,  0.0750, -0.0629,\n",
              "          0.1925, -0.0004]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module子类 --> Sequential类"
      ],
      "metadata": {
        "id": "_Mnk-LKmRoBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 如果模型的前向计算为简单串联各个层的计算时，可以用Sequential来更简单的定义模型\n",
        "# 这个是Sequential大概的运行逻辑\n",
        "class MySequential(nn.Module):\n",
        "  from collections import OrderedDict\n",
        "  def __init__(self, *args):\n",
        "    super(MySequential, self).__init__()\n",
        "    if len(args) == 1 and isinstance(args[0], OrderedDict):\n",
        "      for key, module in args[0].items():\n",
        "        self.add_module(key, module)\n",
        "    else:\n",
        "      for idx, module in enumerate(args):\n",
        "        self.add_module(str(idx), module)\n",
        "\n",
        "  def forward(self, input):\n",
        "    for module in self._modules.values():\n",
        "      input = module(input)\n",
        "    return input"
      ],
      "metadata": {
        "id": "tnDiY19SRsdV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = MySequential(\n",
        "    nn.Linear(784, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 10)\n",
        ")\n",
        "\n",
        "print(net)\n",
        "net(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_4tQ6IHYYlq",
        "outputId": "34b4283a-59a3-4a58-9f4c-a85450e52a08"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MySequential(\n",
            "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1671, -0.3619,  0.1082,  0.2003, -0.0128, -0.1814,  0.2324, -0.1886,\n",
              "          0.0399,  0.0446],\n",
              "        [ 0.0598, -0.3004,  0.0802,  0.0998,  0.1987, -0.3012,  0.3091, -0.0998,\n",
              "          0.0252, -0.1380]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module子类 --> ModuleList类"
      ],
      "metadata": {
        "id": "3KTCy0fSYz6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ModuleList 接受一个子模块的列表作为输入，像list一样可以append和extend\n",
        "\n",
        "net = nn.ModuleList([nn.Linear(784,256), nn.ReLU()])\n",
        "net.append(nn.Linear(256,10))\n",
        "print(net[-1])\n",
        "print(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwZECg_KY4Bp",
        "outputId": "bb1fdcdd-eb11-4dc4-8485-22b946b6d9ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=256, out_features=10, bias=True)\n",
            "ModuleList(\n",
            "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module子类 --> ModuleDict类"
      ],
      "metadata": {
        "id": "mqmtjWmMbIjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = nn.ModuleDict({\n",
        "    'linear': nn.Linear(784,256),\n",
        "    'act': nn.ReLU(),\n",
        "})\n",
        "net['output'] = nn.Linear(256,10)\n",
        "print(net['linear'])\n",
        "print(net.output)\n",
        "print(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hx0a2tQnbM5B",
        "outputId": "8c38b524-bb5a-4047-c732-fca73eed8efa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=784, out_features=256, bias=True)\n",
            "Linear(in_features=256, out_features=10, bias=True)\n",
            "ModuleDict(\n",
            "  (linear): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (act): ReLU()\n",
            "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    }
  ]
}